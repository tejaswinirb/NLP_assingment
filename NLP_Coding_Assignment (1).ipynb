{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Implement the CKY algorithm and test it with your converted L1 grammar.\n"
      ],
      "metadata": {
        "id": "D1CMYN_neEs0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cky_parse(grammar, sentence):\n",
        "  \"\"\"\n",
        "  Implements the CKY parsing algorithm.\n",
        "\n",
        "  Args:\n",
        "    grammar: A dictionary representing the CNF grammar.\n",
        "    sentence: A list of words representing the input sentence.\n",
        "\n",
        "  Returns:\n",
        "    A list of parse trees, where each parse tree is a tuple of the form (non-terminal, left index, right index, children).\n",
        "  \"\"\"\n",
        "\n",
        "  # Initialize the chart.\n",
        "  chart = [[[] for _ in range(len(sentence) + 1)] for _ in range(len(sentence) + 1)]\n",
        "\n",
        "  # Fill in the base cases.\n",
        "  for i, word in enumerate(sentence):\n",
        "    for non_terminal, productions in grammar.items():\n",
        "      if word in productions:\n",
        "        chart[i][i + 1].append((non_terminal, i, i + 1, []))\n",
        "\n",
        "  # Iterate over the chart, filling in the cells.\n",
        "  for length in range(2, len(sentence) + 1):\n",
        "    for start in range(len(sentence) - length + 1):\n",
        "      end = start + length\n",
        "      for split in range(start + 1, end):\n",
        "        left_children = chart[start][split]\n",
        "        right_children = chart[split][end]\n",
        "\n",
        "        for left_non_terminal, left_start, left_end, _ in left_children:\n",
        "          for right_non_terminal, right_start, right_end, _ in right_children:\n",
        "            for non_terminal, productions in grammar.items():\n",
        "              if f\"{left_non_terminal} {right_non_terminal}\" in productions:\n",
        "                chart[start][end].append((non_terminal, left_start, right_end, [(left_non_terminal, left_start, left_end, []), (right_non_terminal, right_start, right_end, [])]))\n",
        "\n",
        "  # Return the parse trees for the start symbol.\n",
        "  return chart[0][len(sentence)]\n",
        "\n",
        "# Define the CNF grammar (converted L1 grammar).\n",
        "cnf_grammar = {\n",
        "    \"S\": [\"NP VP\", \"Aux NP VP\", \"VP\", \"V NP\", \"VP PP\", \"V\", \"NP\", \"Det N\", \"Det\", \"N\", \"V\", \"P\"],\n",
        "    \"NP\": [\"P\", \"Det N\", \"Det N PP\", \"PRP\"],\n",
        "    \"VP\": [\"V\", \"V NP\", \"V NP PP\", \"V PP\", \"VP\"],\n",
        "    \"PP\": [\"P NP\"],\n",
        "    \"Det\": [\"the\", \"a\"],\n",
        "    \"N\": [\"man\", \"park\", \"dog\"],\n",
        "    \"PRP\": [\"I\"],\n",
        "    \"P\": [\"in\", \"with\", \"on\", \"by\", \"to\"],\n",
        "    \"V\": [\"saw\", \"ate\", \"walked\", \"ran\"],\n",
        "    \"Aux\": [\"did\", \"does\", \"do\"]\n",
        "}\n",
        "\n",
        "# Test the CKY algorithm with the converted L1 grammar.\n",
        "sentence = \"the man saw the dog\".split()\n",
        "parse_trees = cky_parse(cnf_grammar, sentence)\n",
        "\n",
        "for tree in parse_trees:\n",
        "  print(tree)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kShycCkiqrk3",
        "outputId": "02ce2631-8ab9-43c8-9257-d50e327640a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('S', 0, 5, [('NP', 0, 2, []), ('VP', 2, 5, [])])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. write the CKY algorithm so that it can accept grammars that contain unit productions.\n"
      ],
      "metadata": {
        "id": "xz9dhaz0sT7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cky_parse_with_unit_productions(grammar, sentence):\n",
        "  \"\"\"\n",
        "  Implements the CKY parsing algorithm, allowing for grammars that contain unit productions.\n",
        "\n",
        "  Args:\n",
        "    grammar: A dictionary representing the CNF grammar.\n",
        "    sentence: A list of words representing the input sentence.\n",
        "\n",
        "  Returns:\n",
        "    A list of parse trees, where each parse tree is a tuple of the form (non-terminal, left index, right index, children).\n",
        "  \"\"\"\n",
        "\n",
        "  # Initialize the chart.\n",
        "  chart = [[[] for _ in range(len(sentence) + 1)] for _ in range(len(sentence) + 1)]\n",
        "\n",
        "  # Fill in the base cases.\n",
        "  for i, word in enumerate(sentence):\n",
        "    for non_terminal, productions in grammar.items():\n",
        "      if word in productions:\n",
        "        chart[i][i + 1].append((non_terminal, i, i + 1, []))\n",
        "\n",
        "  # Iterate over the chart, filling in the cells.\n",
        "  for length in range(2, len(sentence) + 1):\n",
        "    for start in range(len(sentence) - length + 1):\n",
        "      end = start + length\n",
        "      for split in range(start + 1, end):\n",
        "        left_children = chart[start][split]\n",
        "        right_children = chart[split][end]\n",
        "\n",
        "        for left_non_terminal, left_start, left_end, _ in left_children:\n",
        "          for right_non_terminal, right_start, right_end, _ in right_children:\n",
        "            for non_terminal, productions in grammar.items():\n",
        "              if f\"{left_non_terminal} {right_non_terminal}\" in productions:\n",
        "                chart[start][end].append((non_terminal, left_start, right_end, [(left_non_terminal, left_start, left_end, []), (right_non_terminal, right_start, right_end, [])]))\n",
        "              elif f\"{left_non_terminal} -> {right_non_terminal}\" in productions:\n",
        "                chart[start][end].append((non_terminal, left_start, right_end, [(left_non_terminal, left_start, right_end, [])]))\n",
        "              elif f\"{right_non_terminal} -> {left_non_terminal}\" in productions:\n",
        "                chart[start][end].append((non_terminal, left_start, right_end, [(right_non_terminal, left_start, right_end, [])]))\n",
        "\n",
        "  # Return the parse trees for the start symbol.\n",
        "  return chart[0][len(sentence)]\n",
        "\n",
        "# Test the CKY algorithm with the provided grammar and sentence.\n",
        "grammar = {\n",
        "    \"S\": [\"NP VP\", \"Aux NP VP\", \"VP\", \"V NP\", \"VP PP\", \"V\", \"NP\", \"Det N\", \"Det\", \"N\", \"V\", \"P\"],\n",
        "    \"NP\": [\"P\", \"Det N\", \"Det N PP\", \"PRP\"],\n",
        "    \"VP\": [\"V\", \"V NP\", \"V NP PP\", \"V PP\", \"VP\"],\n",
        "    \"PP\": [\"P NP\"],\n",
        "    \"Det\": [\"the\", \"a\"],\n",
        "    \"N\": [\"man\", \"park\", \"dog\"],\n",
        "    \"PRP\": [\"I\"],\n",
        "    \"P\": [\"in\", \"with\", \"on\", \"by\", \"to\"],\n",
        "    \"V\": [\"saw\", \"ate\", \"walked\", \"ran\"],\n",
        "    \"Aux\": [\"did\", \"does\", \"do\"],\n",
        "    \"Aux -> V\": [\"does\", \"do\"]  # Example of unit production\n",
        "}\n",
        "\n",
        "sentence = \"the man saw the dog\".split()\n",
        "parse_trees = cky_parse_with_unit_productions(grammar, sentence)\n",
        "\n",
        "for tree in parse_trees:\n",
        "  print(tree)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jNBOGbWrXpM",
        "outputId": "9215f740-af24-4312-e47d-6d6aed327d1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('S', 0, 5, [('NP', 0, 2, []), ('VP', 2, 5, [])])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement an ELIZA-like program, using substitutions. You might want to choose a different domain than a Rogerian psychologist, although keep in mind that you would need a domain in which your program can legitimately engage in a lot of simple repetition."
      ],
      "metadata": {
        "id": "08-aw4rqvgZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import re\n",
        "\n",
        "# Define a dictionary of substitutions.\n",
        "substitutions = {\n",
        "    \"I\": \"you\",\n",
        "    \"me\": \"you\",\n",
        "    \"my\": \"your\",\n",
        "    \"myself\": \"yourself\",\n",
        "    \"you\": \"I\",\n",
        "    \"your\": \"my\",\n",
        "    \"yours\": \"mine\",\n",
        "    \"yourself\": \"myself\",\n",
        "    \"am\": \"are\",\n",
        "    \"are\": \"am\",\n",
        "    \"was\": \"were\",\n",
        "    \"were\": \"was\",\n",
        "    \"have\": \"have\",\n",
        "    \"has\": \"have\",\n",
        "    \"had\": \"had\",\n",
        "    \"do\": \"do\",\n",
        "    \"does\": \"do\",\n",
        "    \"did\": \"did\",\n",
        "    \"will\": \"will\",\n",
        "    \"would\": \"would\",\n",
        "    \"can\": \"can\",\n",
        "    \"could\": \"could\",\n",
        "    \"should\": \"should\",\n",
        "    \"ought\": \"ought\",\n",
        "    \"must\": \"must\",\n",
        "    \"need\": \"need\",\n",
        "    \"feel\": \"feel\",\n",
        "    \"think\": \"think\",\n",
        "    \"believe\": \"believe\",\n",
        "    \"know\": \"know\",\n",
        "    \"understand\": \"understand\",\n",
        "    \"want\": \"want\",\n",
        "    \"desire\": \"desire\",\n",
        "    \"hope\": \"hope\",\n",
        "    \"wish\": \"wish\",\n",
        "    \"dream\": \"dream\",\n",
        "    \"fear\": \"fear\",\n",
        "    \"worry\": \"worry\",\n",
        "    \"doubt\": \"doubt\",\n",
        "    \"regret\": \"regret\",\n",
        "    \"love\": \"love\",\n",
        "    \"hate\": \"hate\",\n",
        "    \"like\": \"like\",\n",
        "    \"dislike\": \"dislike\",\n",
        "    \"trust\": \"trust\",\n",
        "    \"distrust\": \"distrust\",\n",
        "    \"respect\": \"respect\",\n",
        "    \"disrespect\": \"disrespect\",\n",
        "    \"admire\": \"admire\",\n",
        "    \"criticize\": \"criticize\",\n",
        "    \"blame\": \"blame\",\n",
        "    \"praise\": \"praise\",\n",
        "    \"thank\": \"thank\",\n",
        "    \"apologize\": \"apologize\",\n",
        "    \"forgive\": \"forgive\",\n",
        "    \"help\": \"help\",\n",
        "    \"hurt\": \"hurt\",\n",
        "    \"harm\": \"harm\",\n",
        "    \"kill\": \"kill\",\n",
        "    \"die\": \"die\"\n",
        "}\n",
        "\n",
        "# Define a function to transform a sentence using substitutions.\n",
        "def transform_sentence(sentence):\n",
        "    # Split the sentence into words.\n",
        "    words = sentence.split()\n",
        "\n",
        "    # Apply the substitutions to the words.\n",
        "    for i, word in enumerate(words):\n",
        "        if word in substitutions:\n",
        "            words[i] = substitutions[word]\n",
        "\n",
        "    # Join the words back into a sentence.\n",
        "    return \" \".join(words)\n",
        "\n",
        "# Define a function to generate an ELIZA-like response.\n",
        "def generate_response(sentence):\n",
        "    # Transform the sentence using substitutions.\n",
        "    transformed_sentence = transform_sentence(sentence)\n",
        "\n",
        "    # Generate a generic response.\n",
        "    response = \"I see. And how do you feel about that?\"\n",
        "\n",
        "    # If the transformed sentence contains a question, generate a question response.\n",
        "    if \"?\" in transformed_sentence:\n",
        "        response = \"Why do you ask that?\"\n",
        "\n",
        "    # Return the response.\n",
        "    return response\n",
        "\n",
        "# Start the ELIZA-like program.\n",
        "while True:\n",
        "    # Get the user's input.\n",
        "    sentence = input(\"You: \")\n",
        "\n",
        "    # Generate a response.\n",
        "    response = generate_response(sentence)\n",
        "\n",
        "    # Print the response.\n",
        "    print(\"ELIZA:\", response)\n",
        "\n",
        "    # If the user says \"quit\", exit the program.\n",
        "    if sentence == \"quit\":\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLZdj_HYscSp",
        "outputId": "b5bb4b5e-83ba-43ec-df32-3411f36f5b2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: You\n",
            "ELIZA: I see. And how do you feel about that?\n",
            "You: Fine\n",
            "ELIZA: I see. And how do you feel about that?\n",
            "You: great\n",
            "ELIZA: I see. And how do you feel about that?\n",
            "You: quit\n",
            "ELIZA: I see. And how do you feel about that?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the algorithm to convert arbitrary context-free grammars to CNF\n"
      ],
      "metadata": {
        "id": "dG1oSe3puZEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def convert_to_cnf(grammar):\n",
        "  \"\"\"\n",
        "  Converts an arbitrary context-free grammar to Chomsky Normal Form (CNF).\n",
        "\n",
        "  Args:\n",
        "    grammar: A dictionary representing the context-free grammar.\n",
        "\n",
        "  Returns:\n",
        "    A dictionary representing the CNF grammar.\n",
        "  \"\"\"\n",
        "\n",
        "  # Initialize the CNF grammar.\n",
        "  cnf_grammar = {}\n",
        "\n",
        "  # Convert all rules to binary rules.\n",
        "  for non_terminal, productions in grammar.items():\n",
        "    cnf_grammar[non_terminal] = []\n",
        "    for production in productions:\n",
        "      if len(production.split()) == 2:\n",
        "        cnf_grammar[non_terminal].append(production)\n",
        "      else:\n",
        "        new_non_terminal = f\"{non_terminal}_{len(cnf_grammar) + 1}\"\n",
        "        cnf_grammar[new_non_terminal] = [production]\n",
        "        cnf_grammar[non_terminal].append(f\"{new_non_terminal} {production.split()[1]}\")\n",
        "\n",
        "  # Eliminate unit productions.\n",
        "  while True:\n",
        "    changed = False\n",
        "    for non_terminal, productions in cnf_grammar.items():\n",
        "      for production in productions:\n",
        "        if len(production.split()) == 1 and production in cnf_grammar:\n",
        "          for new_production in cnf_grammar[production]:\n",
        "            if new_production not in productions:\n",
        "              cnf_grammar[non_terminal].append(new_production)\n",
        "              changed = True\n",
        "    if not changed:\n",
        "      break\n",
        "\n",
        "  # Return the CNF grammar.\n",
        "  return cnf_grammar\n"
      ],
      "metadata": {
        "id": "eNPb6i6BuMmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a program to compute unsmoothed unigrams and bigrams\n"
      ],
      "metadata": {
        "id": "tvUXSeQmwhjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Generate a random paragraph.\n",
        "paragraph = \" \".join([random.choice([\"the\", \"a\", \"an\", \"in\", \"on\", \"at\", \"of\", \"to\", \"for\", \"by\", \"with\", \"from\", \"up\", \"down\", \"left\", \"right\", \"back\", \"forward\", \"again\", \"once\", \"twice\", \"three times\", \"many times\", \"a few times\", \"no times\", \"all the time\", \"some of the time\", \"most of the time\", \"half of the time\", \"a quarter of the time\", \"three quarters of the time\", \"a third of the time\", \"two thirds of the time\", \"one fifth of the time\", \"four fifths of the time\", \"one sixth of the time\", \"five sixths of the time\", \"one seventh of the time\", \"six sevenths of the time\", \"one eighth of the time\", \"seven eighths of the time\", \"one ninth of the time\", \"eight ninths of the time\", \"one tenth of the time\", \"nine tenths of the time\"]) for _ in range(100)])\n",
        "\n",
        "# Tokenize the paragraph.\n",
        "tokens = paragraph.split()\n",
        "\n",
        "# Compute the unigrams.\n",
        "unigrams = {}\n",
        "for token in tokens:\n",
        "  if token not in unigrams:\n",
        "    unigrams[token] = 0\n",
        "  unigrams[token] += 1\n",
        "\n",
        "# Compute the bigrams.\n",
        "bigrams = {}\n",
        "for i in range(len(tokens) - 1):\n",
        "  bigram = (tokens[i], tokens[i + 1])\n",
        "  if bigram not in bigrams:\n",
        "    bigrams[bigram] = 0\n",
        "  bigrams[bigram] += 1\n",
        "\n",
        "# Print the unigrams and bigrams.\n",
        "print(\"Unigrams:\")\n",
        "for token, count in unigrams.items():\n",
        "  print(f\"{token}: {count}\")\n",
        "\n",
        "print(\"\\nBigrams:\")\n",
        "for bigram, count in bigrams.items():\n",
        "  print(f\"{bigram}: {count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZOFUQLGwPW4",
        "outputId": "98003b73-7024-487e-ddca-dd1c1995eb00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigrams:\n",
            "on: 4\n",
            "a: 10\n",
            "few: 2\n",
            "times: 9\n",
            "three: 5\n",
            "quarters: 3\n",
            "of: 41\n",
            "the: 42\n",
            "time: 41\n",
            "back: 3\n",
            "with: 2\n",
            "half: 2\n",
            "for: 1\n",
            "left: 4\n",
            "by: 7\n",
            "in: 3\n",
            "one: 14\n",
            "seventh: 6\n",
            "from: 4\n",
            "quarter: 3\n",
            "third: 3\n",
            "two: 4\n",
            "thirds: 4\n",
            "four: 3\n",
            "fifths: 3\n",
            "again: 2\n",
            "five: 1\n",
            "sixths: 1\n",
            "eight: 1\n",
            "ninths: 1\n",
            "some: 3\n",
            "ninth: 3\n",
            "seven: 1\n",
            "eighths: 1\n",
            "six: 2\n",
            "sevenths: 2\n",
            "right: 4\n",
            "an: 2\n",
            "twice: 3\n",
            "eighth: 2\n",
            "all: 1\n",
            "no: 2\n",
            "up: 2\n",
            "many: 3\n",
            "forward: 3\n",
            "sixth: 1\n",
            "at: 1\n",
            "once: 1\n",
            "fifth: 2\n",
            "\n",
            "Bigrams:\n",
            "('on', 'a'): 1\n",
            "('a', 'few'): 2\n",
            "('few', 'times'): 2\n",
            "('times', 'three'): 1\n",
            "('three', 'quarters'): 3\n",
            "('quarters', 'of'): 3\n",
            "('of', 'the'): 40\n",
            "('the', 'time'): 41\n",
            "('time', 'back'): 2\n",
            "('back', 'with'): 1\n",
            "('with', 'a'): 1\n",
            "('a', 'half'): 1\n",
            "('half', 'of'): 2\n",
            "('time', 'with'): 1\n",
            "('with', 'for'): 1\n",
            "('for', 'left'): 1\n",
            "('left', 'by'): 1\n",
            "('by', 'in'): 2\n",
            "('in', 'one'): 1\n",
            "('one', 'seventh'): 6\n",
            "('seventh', 'of'): 6\n",
            "('time', 'on'): 2\n",
            "('on', 'from'): 1\n",
            "('from', 'by'): 1\n",
            "('by', 'a'): 1\n",
            "('a', 'quarter'): 3\n",
            "('quarter', 'of'): 3\n",
            "('time', 'a'): 3\n",
            "('a', 'third'): 3\n",
            "('third', 'of'): 3\n",
            "('time', 'left'): 1\n",
            "('left', 'two'): 1\n",
            "('two', 'thirds'): 4\n",
            "('thirds', 'of'): 4\n",
            "('time', 'four'): 2\n",
            "('four', 'fifths'): 3\n",
            "('fifths', 'of'): 3\n",
            "('time', 'again'): 2\n",
            "('again', 'five'): 1\n",
            "('five', 'sixths'): 1\n",
            "('sixths', 'of'): 1\n",
            "('time', 'eight'): 1\n",
            "('eight', 'ninths'): 1\n",
            "('ninths', 'of'): 1\n",
            "('time', 'some'): 1\n",
            "('some', 'of'): 3\n",
            "('time', 'one'): 7\n",
            "('one', 'ninth'): 3\n",
            "('ninth', 'of'): 3\n",
            "('time', 'seven'): 1\n",
            "('seven', 'eighths'): 1\n",
            "('eighths', 'of'): 1\n",
            "('time', 'two'): 1\n",
            "('time', 'three'): 2\n",
            "('time', 'six'): 2\n",
            "('six', 'sevenths'): 2\n",
            "('sevenths', 'of'): 2\n",
            "('time', 'right'): 1\n",
            "('right', 'four'): 1\n",
            "('time', 'an'): 1\n",
            "('an', 'a'): 1\n",
            "('time', 'twice'): 1\n",
            "('twice', 'one'): 1\n",
            "('one', 'eighth'): 2\n",
            "('eighth', 'of'): 2\n",
            "('time', 'all'): 1\n",
            "('all', 'the'): 1\n",
            "('again', 'left'): 1\n",
            "('left', 'an'): 1\n",
            "('an', 'no'): 1\n",
            "('no', 'times'): 2\n",
            "('times', 'some'): 1\n",
            "('time', 'by'): 2\n",
            "('by', 'by'): 2\n",
            "('in', 'three'): 1\n",
            "('three', 'times'): 2\n",
            "('times', 'up'): 1\n",
            "('up', 'one'): 1\n",
            "('time', 'many'): 2\n",
            "('many', 'times'): 3\n",
            "('times', 'one'): 1\n",
            "('on', 'forward'): 1\n",
            "('forward', 'two'): 1\n",
            "('by', 'twice'): 1\n",
            "('twice', 'many'): 1\n",
            "('times', 'of'): 1\n",
            "('of', 'one'): 1\n",
            "('one', 'sixth'): 1\n",
            "('sixth', 'of'): 1\n",
            "('back', 'from'): 1\n",
            "('from', 'forward'): 1\n",
            "('forward', 'from'): 1\n",
            "('from', 'left'): 1\n",
            "('left', 'a'): 1\n",
            "('times', 'right'): 2\n",
            "('right', 'no'): 1\n",
            "('right', 'one'): 1\n",
            "('time', 'the'): 1\n",
            "('the', 'from'): 1\n",
            "('from', 'a'): 1\n",
            "('a', 'back'): 1\n",
            "('back', 'a'): 1\n",
            "('time', 'at'): 1\n",
            "('at', 'by'): 1\n",
            "('by', 'forward'): 1\n",
            "('forward', 'up'): 1\n",
            "('up', 'three'): 1\n",
            "('times', 'once'): 1\n",
            "('once', 'right'): 1\n",
            "('right', 'twice'): 1\n",
            "('twice', 'some'): 1\n",
            "('time', 'half'): 1\n",
            "('times', 'two'): 1\n",
            "('time', 'in'): 1\n",
            "('in', 'on'): 1\n",
            "('on', 'one'): 1\n",
            "('one', 'fifth'): 2\n",
            "('fifth', 'of'): 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Named Entity Recognition"
      ],
      "metadata": {
        "id": "zbdReYGk1su6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import spacy\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define the text to be processed\n",
        "text = \"Barack Obama was born in Honolulu, Hawaii.\"\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Extract the named entities\n",
        "for entity in doc.ents:\n",
        "  print(f\"{entity.text}: {entity.label_}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KRxgSWb1iMT",
        "outputId": "883ad0b3-0f30-47f1-e472-2f6a5d9cdd50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Barack Obama: PERSON\n",
            "Honolulu: GPE\n",
            "Hawaii: GPE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF _ IDF\n"
      ],
      "metadata": {
        "id": "fd4c2FZ02PxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize the TF-IDF vectorizer with desired parameters\n",
        "vectorizer = TfidfVectorizer(max_features=5, stop_words='english')\n",
        "\n",
        "# Define the documents\n",
        "documents = [\n",
        "    \"This is the first document.\",\n",
        "    \"This is the second document.\",\n",
        "    \"This document is different.\"\n",
        "]\n",
        "\n",
        "# Fit and transform the documents\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Print the TF-IDF vectors\n",
        "print(X.toarray())\n",
        "\n",
        "# Access the vocabulary of the vectorizer\n",
        "vocabulary = vectorizer.vocabulary_\n",
        "\n",
        "# Print the vocabulary\n",
        "print(vocabulary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UIWz6Tt1x8W",
        "outputId": "b080309b-8f4a-40f0-995d-9c7d51a63956"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         1.         0.        ]\n",
            " [0.         0.50854232 0.861037  ]\n",
            " [0.861037   0.50854232 0.        ]]\n",
            "{'document': 1, 'second': 2, 'different': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use a library like NLTK or spaCy to perform part-of-speech tagging on\n",
        "a sentence. Print out the words along with their corresponding parts of speech"
      ],
      "metadata": {
        "id": "daLBUWNF2dHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define the sentence to be processed\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Process the sentence\n",
        "doc = nlp(sentence)\n",
        "\n",
        "# Print the words and their corresponding parts of speech\n",
        "for token in doc:\n",
        "  print(f\"{token.text}: {token.pos_}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlleiYsy2W_m",
        "outputId": "560adf19-84b1-4f5a-cea9-0fcede0fe66c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The: DET\n",
            "quick: ADJ\n",
            "brown: ADJ\n",
            "fox: NOUN\n",
            "jumps: VERB\n",
            "over: ADP\n",
            "the: DET\n",
            "lazy: ADJ\n",
            "dog: NOUN\n",
            ".: PUNCT\n"
          ]
        }
      ]
    }
  ]
}